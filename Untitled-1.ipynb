{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF for PDF processing\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Normally using pip install unsloth is enough\n",
    "\n",
    "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
    "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Mini projects\\New folder\\venv\\Lib\\site-packages\\unsloth\\__init__.py:139\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# For Gradio HF Spaces?\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# if \"SPACE_AUTHOR_NAME\" not in os.environ and \"SPACE_REPO_NAME\" not in os.environ:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\n\u001b[32m    140\u001b[39m libcuda_dirs = \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Version(triton.__version__) >= Version(\u001b[33m\"\u001b[39m\u001b[33m3.0.0\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  # Should print True\n",
    "print(\"CUDA Version:\", torch.version.cuda)  # Should match your CUDA version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Using GPU: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_17956\\3968186665.py:14: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  A = torch.randn(size, size, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication completed in 0.277400 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Create two random matrices on GPU\n",
    "size = 1000  # Adjust size for more stress\n",
    "A = torch.randn(size, size, device=device)\n",
    "B = torch.randn(size, size, device=device)\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "C = torch.matmul(A, B)  # Matrix multiplication\n",
    "torch.cuda.synchronize()  # Wait for GPU to finish\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Matrix multiplication completed in {end_time - start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- GOOGLE DRIVE UPLOAD CONFIG ----\n",
    "SCOPES = [\"https://www.googleapis.com/auth/drive\"]\n",
    "SERVICE_ACCOUNT_FILE = \"your_service_account.json\"  # Add your Google Cloud service account JSON here\n",
    "\n",
    "# Authenticate with Google Drive\n",
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "drive_service = build(\"drive\", \"v3\", credentials=credentials)\n",
    "\n",
    "# ---- STREAMLIT UI ----\n",
    "st.title(\"ðŸ“„ PDF Upload & Fine-Tune AI Chatbot\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF\", type=[\"pdf\"])\n",
    "if uploaded_file is not None:\n",
    "    pdf_path = f\"./{uploaded_file.name}\"\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.getbuffer())\n",
    "\n",
    "    st.success(\"âœ… PDF uploaded successfully!\")\n",
    "\n",
    "    # ---- UPLOAD TO GOOGLE DRIVE ----\n",
    "    file_metadata = {\"name\": uploaded_file.name, \"parents\": [\"your_drive_folder_id\"]}\n",
    "    media = MediaFileUpload(pdf_path, mimetype=\"application/pdf\")\n",
    "    drive_service.files().create(body=file_metadata, media_body=media, fields=\"id\").execute()\n",
    "    st.success(\"ðŸ“‚ PDF saved to Google Drive!\")\n",
    "\n",
    "    # ---- EXTRACT TEXT FROM PDF ----\n",
    "    def extract_text(pdf_path):\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    extracted_text = extract_text(pdf_path)\n",
    "    st.info(f\"ðŸ“œ Extracted {len(extracted_text)} characters from PDF.\")\n",
    "\n",
    "    # ---- CREATE DATASET FOR FINE-TUNING ----\n",
    "    df_combined = Dataset.from_pandas({\"text\": [extracted_text]})\n",
    "\n",
    "    # ---- LOAD & FINE-TUNE LLM ----\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\"\n",
    "    max_seq_length = 2048\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0\n",
    "    )\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_seq_length)\n",
    "\n",
    "    dataset = df_combined.map(tokenize_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    st.info(\"ðŸ”„ Fine-tuning the model... This may take a while.\")\n",
    "    trainer.train()\n",
    "    st.success(\"ðŸŽ‰ Model fine-tuning complete!\")\n",
    "\n",
    "    # ---- SAVE FINE-TUNED MODEL ----\n",
    "    model_path = \"./fine_tuned_model\"\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    st.success(\"ðŸ“¦ Fine-tuned model saved!\")\n",
    "\n",
    "    # ---- CHAT INTERFACE ----\n",
    "    st.subheader(\"ðŸ’¬ Ask the AI a Question\")\n",
    "\n",
    "    user_input = st.text_input(\"Enter your question:\")\n",
    "    if st.button(\"Ask\"):\n",
    "        if user_input:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "            inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_length=500)\n",
    "\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            st.write(\"ðŸ¤– AI:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testcudaunsloth",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
